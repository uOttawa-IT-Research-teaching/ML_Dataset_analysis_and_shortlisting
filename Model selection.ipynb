{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b444bbc2-d237-4d5d-88fc-5b46f185b7bc",
   "metadata": {},
   "source": [
    "# Dataset analysis, model shortlisting, and model determination\n",
    "\n",
    "In all of the past notebooks we have looked at many different models and many different datasets. However, one big question remains: When to pick which model?\n",
    "\n",
    "That is of course not an easy question to answer. We have indeed seen a few notebooks where we tried to apply a specific machine learning algorithm and it would just fail. For example the with k-nearest neighbours, we tried to classify books and failed to produce any accurate predictions for the genres. However when we tried the same thing with Naive Bayes, it worked really well!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cff71b-dfc2-41ff-a0b8-6e4b2d62f355",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "\n",
    "You will be able to:\n",
    "* Convert a Jupyter to a regular Python script for running large projects on a compute cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59fa538-cccf-455d-a206-b3de7e85afa3",
   "metadata": {},
   "source": [
    "## What you will need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c744f018-cb75-4892-8ac7-3de9d4c1fd9e",
   "metadata": {},
   "source": [
    "## Choices!\n",
    "\n",
    "We have seen many models in this set of modules\n",
    "* linear regression\n",
    "* random forest\n",
    "* K-nearest neighbours\n",
    "* Support vector machines\n",
    "* Image inferencing\n",
    "* Naïve Bayes\n",
    "* Transfer learning\n",
    "* Deep and Convolutional Neural Networks (DNN/CNN)\n",
    "* Recursive neural networks and long-short term memory\n",
    "* Natural language processing\n",
    "\n",
    "It is not usually clear which model would work the best for your dataset. Even with natural language processing, which tells you right in the name what it is for, is not clear because NLP is more of an umbrella term of a collection of models. For example, with Naïve Bayes, we already did some natural language processing by classifying books in genres based on the book summaries. For the NLP notebooks we instead used an unsupervised model for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49951789-d115-4c55-bbca-e24e560516b4",
   "metadata": {},
   "source": [
    "## Cleaning your data\n",
    "\n",
    "There is no easy flowchart that will help you pick the right model. However, the very first step is always to clean your data. This is an important step that many online tutorials will leave out because it's sometimes not considered part of machine learning, but it is! Without good input data, no model will be able to work well.\n",
    "\n",
    "The next step is to normalize your data if you can. Many model perform better if the input data is between 0 and 1 or between -1 and 1. We saw this with k-nearest neighbours for example where the one of the features had numbers in the thousands and the other in the tens. The bigger numbers simply made the algorithm pretty much disregard the features with the smaller numbers.\n",
    "\n",
    "In practice, you will likely try out a few different models to see how they perform as we did with the books where we first used k-nearest neighhours with poor results and then Naïve Bayes with much better results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee41991-9dad-4e62-8be1-80aa3c1785aa",
   "metadata": {},
   "source": [
    "## Model shortlisting\n",
    "Once you have clean data however, there are a few things you can do to narrow things down the choice of your model. The first question is what are you want the output to be. Do you want to predict a continuous variable, for example age or income\" If so then you need a model that does regression such as linear regression. If you are looking you yes/no answers, for example patient does or does not have this decease, then you may want models like the random forest. If you want to divide your data into groups then you need to use an algorithm that can classify such as K-nearest neighbours or Naive Bayes. There is certainly some overlap between yes/no questions and classification since yes/no questions are just classifying your data into two groups.\n",
    "\n",
    "Another thing you need to consider is the input data. Do you have labelled data or not? That determines if you can use supervised or unsupervised models.\n",
    "\n",
    "Another important question is how much computational resources do you have available. Model like k-nearest neighbours are very fast to train however they become slow to use when the dataset grows because they need to check every neighbour whenever you ask it to classify a new data point.\n",
    "\n",
    "Do you need explainability? Some models are black boxes like the various neural networks, others like decision trees or support vector machines will let you see why it is classifying the way it is to some extent.\n",
    "\n",
    "Let's go over the pros and cons of each model that we have seen so far."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240fdb60-c785-4b3f-8856-5f1c48ded77a",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "**Pros:**\n",
    "\n",
    "1. **Interpretability**: The coefficients in a linear regression model can be easily interpreted as the change in the response variable per unit change in each predictor, while controlling for all other predictors.\n",
    "2. **Simple to implement**: Linear regression is one of the most straightforward machine learning algorithms to implement, requiring only basic mathematical operations and no complex hyperparameter tuning.\n",
    "3. **Fast training time**: The optimization process for linear regression models is typically very fast, making it suitable for large datasets or when working with limited computational resources.\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "1. **Assumes linearity**: Linear regression assumes a linear relationship between the predictors and response variable, which may not always be true in real-world data.\n",
    "2. **Sensitive to outliers**: Linear regression can be heavily influenced by outliers in the data, leading to poor model performance or even incorrect conclusions if not properly handled.\n",
    "3. **Not suitable for categorical variables**: By default, linear regression is designed to work with numerical predictors and response variables. It's not well-suited for modeling relationships involving categorical variables (e.g., text data, binary flags), which often require more specialized techniques like logistic regression or decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baa94bd-2d87-4ff3-a98d-d579185eda4c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### K-nearest Neighbours\n",
    "1. **Simple to implement**: KNN is a straightforward algorithm that only looks at nearest neighbours as the name implies. \n",
    "2. **Interpretable results**: KNN provides transparent and interpretable results, allowing you to understand why a particular prediction was made and what features were most influential in making that decision.\n",
    "3. **Good for small datasets**: KNN is relatively fast and efficient when dealing with smaller datasets (e.g., fewer than 10,000 samples), as it doesn't require significant computational resources for small datasets.\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "1. **Sensitive to noise**: KNN can be heavily influenced by noisy or outlier data points in the training set, which may lead to poor performance on test sets.\n",
    "2. **Computational complexity increases with k**: As you increase the value of k (the number of nearest neighbours), the computational cost of calculating distances and finding the closest neighbours also grows, making it less suitable for large datasets.\n",
    "3. **No explicit model learned**: KNN doesn't learn a explicit model or representation of the data; instead, it relies on the distance metric to make predictions. This can limit its ability to generalize well in complex domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9974d12f-a5ba-47aa-aa7e-3120281fbc95",
   "metadata": {},
   "source": [
    "### Support Vector Machines\n",
    "**Pros:**\n",
    "\n",
    "1. **Good performance on high-dimensional data**: SVMs can handle datasets with a large number of features.\n",
    "2. **Robust to noise and outliers**: SVMs can use the kernel trick that allows them to map the input space into a higher-dimensional feature space where it's easier to find linear separators. This makes them more robust to noisy or outlier data points and in particular when using soft margins.\n",
    "3. **Interpretable results**: The decision boundary of an SVM can occasionally be visualized, which can provide valuable insights about the relationships between features and classes. Whether or not the boundary can be visualized depends on the dimensions of the input data where you can plot maybe five dimensions if you add colours and animation.\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "1. **Computational complexity**: Training an SVM can be computationally expensive, especially for large datasets. This is because the algorithm scales between quadratically to the third power with the number of data points. That is, if you double the number of data points in your input data training will take between four and eight times as long.\n",
    "2. **Requires careful tuning of hyperparameters**: The performance of an SVM depends heavily on the choice of kernel function, regularization parameter, and other hyperparameters. Finding the right combination of these parameters can be time-consuming and requires trial and error or just experience.\n",
    "3. **Not suitable for large datasets with many classes**: While SVMs are good at handling high-dimensional data, they're not well-suited for problems where there are a very large number of classes because at its core, SVMs only do binary classification. While it is possible to extend SVMs it usually does so by dividing the problem into multiple binary classification tasks and thus "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d688904e-35ae-47a8-9b3b-93f7175cf0e9",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "\n",
    "**Pros:**\n",
    "\n",
    "1. **Simple to implement**: Naive Bayes is a straightforward algorithm that can be easily implemented, even for those without extensive machine learning experience.\n",
    "2. **Fast training time**: Naive Bayes models require minimal computational resources during the training process, making them suitable for large datasets or real-time applications where speed matters.\n",
    "3. **Good performance on binary classification problems**: Naive Bayes is particularly effective when dealing with binary classification tasks (e.g., spam vs. not spam emails)\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "1. **Assumes independence of features**: The \"naive\" part of the algorithm refers to its assumption that all features are independent, which is often unrealistic in real-world scenarios where feature correlations exist.\n",
    "2. **Sensitive to class imbalance**: Naive Bayes can be affected by class imbalance issues (e.g., when one class has significantly more instances than others), leading to biased predictions and poor performance on minority classes.\n",
    "3. **Not suitable for continuous-valued features**: Naive Bayes is primarily designed for categorical data and may not perform well when dealing with continuous-valued features that require more sophisticated handling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7228be3f-d417-487b-81c8-ff99b9991d17",
   "metadata": {},
   "source": [
    "### Deep and Convolutional Neural Networks\n",
    "\n",
    "**Pros:**\n",
    "\n",
    "1. **State-of-the-Art Performance**: DNNs and CNNs have been shown to achieve state-of-the-art performance on many benchmark datasets, often surpassing traditional machine learning algorithms like decision trees or support vector machines.\n",
    "2. **Ability to Learn Complex Patterns**: The hierarchical structure of CNNs and DNNs allows them to learn complex patterns and relationships in data that may not be easily captured by shallower models. This is particularly useful for tasks involving high-dimensional input spaces or noisy data.\n",
    "3. **Flexibility and Customizability**: CNNs and DNNs can be designed with various architectures, activation functions, and optimization algorithms, making them highly flexible and customizable to specific problem domains where CNNs in particular excel in image recognition tasks.\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "1. **High Computational Requirements**: Training large-scale DNNs and CNNs requires significant computational resources (e.g., GPUs) and memory, which can make it challenging for researchers or practitioners without access to such infrastructure.\n",
    "2. **Overfitting Risk**: The complexity of DNNs makes them prone to overfitting, where the model becomes too specialized in the training data and fails to generalize well to new examples.\n",
    "3. **Interpretability Challenges**: Due to their complex internal representations, it can be difficult to understand why a DNN or CNN is making certain predictions or decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249d9a1b-256a-42e2-8126-f4bb09c69b57",
   "metadata": {},
   "source": [
    "### Image Inferencing\n",
    "\n",
    "Image inferencing almost always involves convolutional networks. If you looking to do image recognition, CNN is likely what you want. It is however a computationally expensive technique. There are still other technique. For example, for just detecting markers a CNN might be overkill and you could rely on libraries like [OpenCV](https://docs.opencv.org/4.x/index.html) which take a more traditional approach using impressive image manipulation techniques.\n",
    "\n",
    "Another way to reduce the computational cost of CNN is by scaling the input image down to as small as a size as you can get away wtth since reducing the both dimensions of an image by a factor two means you'll have four times less pixels as input and depending on the algorithm, your computation time might reduce by much more than just a factor four."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f96f666-9f24-491d-82f9-9ca048c31d9f",
   "metadata": {},
   "source": [
    "### Transfer Learning\n",
    "\n",
    "**Pros:**\n",
    "\n",
    "1. **Efficient Training**: Transfer learning allows you to leverage pre-trained models on large datasets, saving time and computational resources during training.\n",
    "2. **Improved Performance**: By transferring knowledge from a source task to a target task, transfer learning can often lead to better performance, especially when the target task has limited data.\n",
    "3. **Domain Adaptation**: Transfer learning is useful for adapting models trained on one domain to perform well on a different but related domain, reducing the need for collecting labeled data in the target domain.\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "    Overfitting: Transfer learning can sometimes lead to overfitting, especially if the source and target domains are too dissimilar or if the transferred knowledge is not properly adapted to the target task.\n",
    "    Negative Transfer: In some cases, transferring knowledge from a source task can actually hurt performance on the target task if the source task is too different or irrelevant.\n",
    "    Limited Flexibility: Pre-trained models may not always be suitable for all target tasks, limiting the flexibility of transfer learning in certain scenarios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badd1f84-cb25-44c4-9e1f-2ed64320ebc0",
   "metadata": {},
   "source": [
    "### Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a978dd-7155-4af9-b7bd-0ea417fafa7f",
   "metadata": {},
   "source": [
    "Example 1 - Image classification: You want to build a model that can classify new images as either cats or dogs. In this case, image inferencing might be suitable for training a convolutional neural network (CNN) on the data. CNNs are particularly good at processing visual data.\n",
    "\n",
    "Example 2 - Stock market prediction: You want to predict stock prices based on historical financial data. In this case, linear regression might be a suitable algorithm as it can model continuous variables and make predictions for new inputs. Additionally, preprocessing steps like scaling features (e.g., price changes) will help improve the model's performance.\n",
    "\n",
    "Example 4 - Sentiment analysis: You have text data from social media posts that you want to analyze for positive or negative sentiment. In this case, natural language processing techniques like recurrent neural networks (RNNs) and long-short term memory (LSTM) models would be suitable due to their ability to process sequential data effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48900cd8-fa4e-4650-9492-913a0359eee5",
   "metadata": {},
   "source": [
    "## Running large models\n",
    "\n",
    "When the size of your dataset or the complexity of your model grows, you will run into the limitations of your computer. You could of course buy bigger hard drives, more memory, and more powerful GPUs, but a better way to go about this is to use a dedicated compute cluster. This can either be a cluster from the Digital Research Alliance of Canada, your group's own server, or any of the commercial cloud offerings.\n",
    "\n",
    "While many of these offerings will come with JupyterHub interfaces of their own, it is not a very efficient way of doing things because Jupyter notebooks are not designed for efficiency. For example, they don't end when your code finishes running. For dedicated commercial clouds that means you will keep paying. For compute clusters, it will mean you keep resources tied up such that other users cannot use them.\n",
    "\n",
    "The solution to this is to use a standalone script that you start from the command-line. It will run from start to finish and save the results to disk. Afterwards, you will be able to load the saved model or results and analyze them on your own machine since those tasks would typically require much fewer resources than for training a model. This does mean you need to make a adjustment to your workflow. Before, you likely had a single notebook with both the training and the analysis in it which you will now have to split in two. There should be one notebook for training the model and writing the relevant files to disk and another notebook that loads those files and uses the trained model.\n",
    "\n",
    "There are a few ways of making a Jupyter notebook into a standalone entity. One of the easiest ways is to simply export from Jupyter by going to `File` → `Save and export notebook as...` → `Executable Script`.\n",
    "\n",
    "The resulting Python script is then something you can run from the command-line:\n",
    "```\n",
    "python nameofscript.py\n",
    "```\n",
    "\n",
    "This makes it suitable for running on remote servers. For example, for running on one of the Alliance clusters with a nice big GPU, you would create a submission script that look roughly like this:\n",
    "```\n",
    "#!/bin/bash\n",
    "#SBATCH --account=def-someuser    # Account name of your allocation. Usually def-[name of supervisor]\n",
    "#SBATCH --gpus-per-node=1         # Number of GPUs\n",
    "#SBATCH --mem=4000M               # Memory per node\n",
    "#SBATCH --time=0-03:00            # Maximum run time, in days - hours : minutes\n",
    "\n",
    "module load python                # Load the latest version of Python\n",
    "source ~/tf/bin/activate          # Load the Tensorflow or Torch environment\n",
    "python nameofscript.py            # Run your script\n",
    "```\n",
    "See https://docs.alliancecan.ca/wiki/Using_GPUs_with_Slurm for GPU job submission and https://docs.alliancecan.ca/wiki/TensorFlow and https://docs.alliancecan.ca/wiki/PyTorch for loading Tensorflow or PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b535827-f942-4305-aa82-b0298eb308c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
