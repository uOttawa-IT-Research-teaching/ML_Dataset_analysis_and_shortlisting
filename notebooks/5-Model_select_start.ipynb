{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b444bbc2-d237-4d5d-88fc-5b46f185b7bc",
   "metadata": {},
   "source": [
    "# Dataset analysis, model shortlisting, and model determination: Model shortlisting and Optimization\n",
    "\n",
    "In all of the past notebooks we have looked at many different models and many different use cases involding the datasets. However, two big questions remain: <br>\n",
    "### How to pick the models when faced with more than one choice? <br>\n",
    "### What options are there when a selected model reaches it's full potential?\n",
    "\n",
    "Those are of course not easy questions to answer. In past workshops, we tried to apply a specific machine learning algorithm and it would just fail. For example the with k-nearest neighbours, we tried to classify books and failed to produce any accurate predictions for the genres. However when we tried the same thing with Naive Bayes, it worked really well!\n",
    "\n",
    "**Notebook 5 of 5**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cff71b-dfc2-41ff-a0b8-6e4b2d62f355",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "\n",
    "You will be able to:\n",
    "* Gain insight to help shortlist the model(s) suitable for your project.\n",
    "* Convert a Jupyter to a regular Python script for running large projects on a compute cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59fa538-cccf-455d-a206-b3de7e85afa3",
   "metadata": {},
   "source": [
    "## What you will need\n",
    "* See the [introduction document](https://uottawa-it-research-teaching.github.io/machinelearning/) for general requirements and how Jupyter notebooks work.\n",
    "* If you want to work on the Digital Research Alliance clusters, you will need an account."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240fdb60-c785-4b3f-8856-5f1c48ded77a",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "**Pros:**\n",
    "\n",
    "1. **Interpretability**: The coefficients in a linear regression model can be easily interpreted as the change in the response variable per unit change in each predictor, while controlling for all other predictors.\n",
    "2. **Simple to implement**: Linear regression is one of the most straightforward machine learning algorithms to implement, requiring only basic mathematical operations and no complex hyperparameter tuning.\n",
    "3. **Fast training time**: The optimization process for linear regression models is typically very fast, making it suitable for large datasets or when working with limited computational resources.\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "1. **Assumes linearity**: Linear regression assumes a linear relationship between the predictors and response variable, which may not always be true in real-world data.\n",
    "2. **Sensitive to outliers**: Linear regression can be heavily influenced by outliers in the data, leading to poor model performance or even incorrect conclusions if not properly handled.\n",
    "3. **Not suitable for categorical variables**: By default, linear regression is designed to work with numerical predictors and response variables. It's not well-suited for modeling relationships involving categorical variables (e.g., text data, binary flags), which often require more specialized techniques like logistic regression or decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa57ac7-6b5d-4b37-b472-4bba7c0de7e8",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "\n",
    "**Pros:**\n",
    "\n",
    "1. **Interpretability**: Decision trees are highly interpretable and can be visualized as flowcharts, making it easy to understand the decision-making process 1.\n",
    "2. **Transparency**: Each decision node clearly shows the criteria used for splitting, which helps in understanding how the model arrives at its predictions.\n",
    "3. **Automatic Feature Selection**: Decision trees automatically select the most important features for splitting, which simplifies the model-building process.\n",
    "4. **Handling Mixed Data Types and Missing values**: They can handle both numerical and categorical data without requiring extensive preprocessingas well as handle missing values naturally, which simplifies data preparation.\n",
    "   \n",
    "**Cons:**\n",
    "\n",
    "1. **Complexity in Large Trees**: As the tree grows larger, it can become complex and harder to interpret, especially if there are many branches and nodes.\n",
    "2. **Overfitting**: Decision trees can easily overfit the training data, especially if they are allowed to grow too deep.\n",
    "3. **Sensitivity to Noise**: Decision trees can be sensitive to noise in the data, which can lead to instability and overfitting.\n",
    "4. **Bias Towards Dominant Classes**: If some classes dominate, the decision tree might be biased towards those classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baa94bd-2d87-4ff3-a98d-d579185eda4c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### K-nearest Neighbours\n",
    "\n",
    "**Pros:**\n",
    "\n",
    "1. **Simple to implement**: KNN is a straightforward algorithm that only looks at nearest neighbours as the name implies. \n",
    "2. **Interpretable results**: KNN provides transparent and interpretable results, allowing you to understand why a particular prediction was made and what features were most influential in making that decision.\n",
    "3. **Good for small datasets**: KNN is relatively fast and efficient when dealing with smaller datasets (e.g., fewer than 10,000 samples), as it doesn't require significant computational resources for small datasets.\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "1. **Sensitive to noise**: KNN can be heavily influenced by noisy or outlier data points in the training set, which may lead to poor performance on test sets.\n",
    "2. **Computational complexity increases with k**: As you increase the value of k (the number of nearest neighbours), the computational cost of calculating distances and finding the closest neighbours also grows, making it less suitable for large datasets.\n",
    "3. **No explicit model learned**: KNN doesn't learn a explicit model or representation of the data; instead, it relies on the distance metric to make predictions. This can limit its ability to generalize well in complex domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9974d12f-a5ba-47aa-aa7e-3120281fbc95",
   "metadata": {},
   "source": [
    "### Support Vector Machines\n",
    "**Pros:**\n",
    "\n",
    "1. **Good performance on high-dimensional data**: SVMs can handle datasets with a large number of features.\n",
    "2. **Robust to noise and outliers**: SVMs can use the kernel trick that allows them to map the input space into a higher-dimensional feature space where it's easier to find linear separators. This makes them more robust to noisy or outlier data points and in particular when using soft margins.\n",
    "3. **Interpretable results**: The decision boundary of an SVM can occasionally be visualized, which can provide valuable insights about the relationships between features and classes. Whether or not the boundary can be visualized depends on the dimensions of the input data where you can plot maybe five dimensions if you add colours and animation.\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "1. **Computational complexity**: Training an SVM can be computationally expensive, especially for large datasets. This is because the algorithm scales between quadratically to the third power with the number of data points. That is, if you double the number of data points in your input data training will take between four and eight times as long.\n",
    "2. **Requires careful tuning of hyperparameters**: The performance of an SVM depends heavily on the choice of kernel function, regularization parameter, and other hyperparameters. Finding the right combination of these parameters can be time-consuming and requires trial and error or just experience.\n",
    "3. **Not suitable for large datasets with many classes**: While SVMs are good at handling high-dimensional data, they're not well-suited for problems where there are a very large number of classes because at its core, SVMs only do binary classification. While it is possible to extend SVMs it usually does so by dividing the problem into multiple binary classification tasks and thus "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d688904e-35ae-47a8-9b3b-93f7175cf0e9",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "\n",
    "**Pros:**\n",
    "\n",
    "1. **Simple to implement**: Naive Bayes is a straightforward algorithm that can be easily implemented, even for those without extensive machine learning experience.\n",
    "2. **Fast training time**: Naive Bayes models require minimal computational resources during the training process, making them suitable for large datasets or real-time applications where speed matters.\n",
    "3. **Good performance on binary classification problems**: Naive Bayes is particularly effective when dealing with binary classification tasks (e.g., spam vs. not spam emails)\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "1. **Assumes independence of features**: The \"naive\" part of the algorithm refers to its assumption that all features are independent, which is often unrealistic in real-world scenarios where feature correlations exist.\n",
    "2. **Sensitive to class imbalance**: Naive Bayes can be affected by class imbalance issues (e.g., when one class has significantly more instances than others), leading to biased predictions and poor performance on minority classes.\n",
    "3. **Not suitable for continuous-valued features**: Naive Bayes is primarily designed for categorical data and may not perform well when dealing with continuous-valued features that require more sophisticated handling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7228be3f-d417-487b-81c8-ff99b9991d17",
   "metadata": {},
   "source": [
    "### Deep and Convolutional Neural Networks\n",
    "\n",
    "**Pros:**\n",
    "\n",
    "1. **State-of-the-Art Performance**: DNNs and CNNs have been shown to achieve state-of-the-art performance on many benchmark datasets, often surpassing traditional machine learning algorithms like decision trees or support vector machines.\n",
    "2. **Ability to Learn Complex Patterns**: The hierarchical structure of CNNs and DNNs allows them to learn complex patterns and relationships in data that may not be easily captured by shallower models. This is particularly useful for tasks involving high-dimensional input spaces or noisy data.\n",
    "3. **Flexibility and Customizability**: CNNs and DNNs can be designed with various architectures, activation functions, and optimization algorithms, making them highly flexible and customizable to specific problem domains where CNNs in particular excel in image recognition tasks.\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "1. **High Computational Requirements**: Training large-scale DNNs and CNNs requires significant computational resources (e.g., GPUs) and memory, which can make it challenging for researchers or practitioners without access to such infrastructure.\n",
    "2. **Overfitting Risk**: The complexity of DNNs makes them prone to overfitting, where the model becomes too specialized in the training data and fails to generalize well to new examples.\n",
    "3. **Interpretability Challenges**: Due to their complex internal representations, it can be difficult to understand why a DNN or CNN is making certain predictions or decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249d9a1b-256a-42e2-8126-f4bb09c69b57",
   "metadata": {},
   "source": [
    "### Image Inferencing\n",
    "\n",
    "Image inferencing almost always involves convolutional networks. If you looking to do image recognition, CNN is likely what you want. It is however a computationally expensive technique. There are still other technique. For example, for just detecting markers a CNN might be overkill and you could rely on libraries like [OpenCV](https://docs.opencv.org/4.x/index.html) which take a more traditional approach using impressive image manipulation techniques.\n",
    "\n",
    "With an understanding of your model, you can use tools such as [openVINO](https://docs.openvino.ai/2025/index.html) for cross-platform inference deployment to optimize models for CPUs and GPUs. A key tool is the Model Optimizer, which takes trained models and prepares them for optimized execution in the Intermediate Representations format understood by the OpenVINO Inference Engine. We can leverage the high-performance inference capabilities of the Inference Engine to run the model on both CPU and GPU targets. Other techniques like model quantization reduce the size of the model without substantially impacting accuracy, important for embedded and edge deployments. As an example, by combining optimizing tools and preparing your data, such as scaling the input image down to as small a size as you can get away with, you'll reduce the pixels as input and gain computational efficiency by much greater than the chosen image scaling factor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f96f666-9f24-491d-82f9-9ca048c31d9f",
   "metadata": {},
   "source": [
    "### Transfer Learning\n",
    "\n",
    "**Pros:**\n",
    "\n",
    "1. **Efficient Training**: Transfer learning allows you to leverage pre-trained models on large datasets, saving time and computational resources during training.\n",
    "2. **Improved Performance**: By transferring knowledge from a source task to a target task, transfer learning can often lead to better performance, especially when the target task has limited data.\n",
    "3. **Domain Adaptation**: Transfer learning is useful for adapting models trained on one domain to perform well on a different but related domain, reducing the need for collecting labeled data in the target domain.\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "1. **Overfitting**: Transfer learning can sometimes lead to overfitting, especially if the source and target domains are too dissimilar or if the transferred knowledge is not properly adapted to the target task.\n",
    "2. **Negative Transfer**: In some cases, transferring knowledge from a source task can actually hurt performance on the target task if the source task is too different or irrelevant.\n",
    "3. **Limited Flexibility**: Pre-trained models may not always be suitable for all target tasks, limiting the flexibility of transfer learning in certain scenarios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badd1f84-cb25-44c4-9e1f-2ed64320ebc0",
   "metadata": {},
   "source": [
    "### Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a978dd-7155-4af9-b7bd-0ea417fafa7f",
   "metadata": {},
   "source": [
    "**Example 1** - Image classification: You want to build a model that can classify new images as either cats or dogs. In this case, image inferencing might be suitable for training a convolutional neural network (CNN) on the data. CNNs are particularly good at processing visual data.\n",
    "\n",
    "**Example 2** - Stock market prediction: You want to predict stock prices based on historical financial data. In this case, linear regression might be a suitable algorithm as it can model continuous variables and make predictions for new inputs. Additionally, preprocessing steps like scaling features (e.g., price changes) will help improve the model's performance.\n",
    "\n",
    "**Example 3** - Sentiment analysis: You have text data from social media posts that you want to analyze for positive or negative sentiment. In this case, natural language processing techniques like recurrent neural networks (RNNs) and long-short term memory (LSTM) models would be suitable due to their ability to process sequential data effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43fe64a-9f68-44e7-ad69-75d2b47ed2f3",
   "metadata": {},
   "source": [
    "## Cascading Models\n",
    "Cascading machine learning models can be a powerful strategy when a single model's full potential is reached. This approach involves using multiple models in a sequence, where each model builds upon the output of the previous one. Here are some key benefits of cascading models:\n",
    "\n",
    "### Enhanced Performance\n",
    "**Combining Strengths**: Cascading allows different models to leverage their unique strengths. For instance, a simpler model can perform initial filtering or preprocessing, while more complex models can handle detailed analysis.\n",
    "\n",
    "**Improved Accuracy**: By combining multiple models, cascading can improve overall accuracy. Each model in the cascade can correct errors made by the previous models, leading to more reliable predictions.\n",
    "\n",
    "### Efficient Computing Resource Utilization\n",
    "**Optimized Computation**: Cascading can optimize computational resources by using lightweight models for initial tasks and reserving more intensive computations for later stages. This is particularly useful in embedded systems and edge computing.\n",
    "\n",
    "**Handling Large Datasets**: Cascading models can efficiently handle large datasets by breaking down the problem into smaller, manageable tasks. This can reduce the computational burden and improve processing speed.\n",
    "\n",
    "### Robustness to Noise and Bias\n",
    "**Noise Reduction**: Initial models in the cascade can filter out noise and irrelevant data, allowing subsequent models to focus on cleaner, more relevant information.\n",
    "\n",
    "**Bias Mitigation**: Cascading can help mitigate bias by combining models with different perspectives. This can lead to more balanced and fair predictions.<br>\n",
    "\n",
    "**Example**: <b>[Addressing variance and noise in the data](https://github.com/uOttawa-IT-Research-teaching/SVM/blob/main/notebooks/MLTS_20240530_SVM_Notebook4SVMRandomForest_EN_1.0.ipynb)</b>\n",
    "\n",
    "Overall, cascading machine learning models offer a versatile and effective approach to enhancing performance, optimizing resources, and improving robustness in various applications. Would you like to explore specific examples or techniques used in cascading models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48900cd8-fa4e-4650-9492-913a0359eee5",
   "metadata": {},
   "source": [
    "## Running large models on high-performance computing platform\n",
    "\n",
    "When the size of your dataset or the complexity of your model grows, you will run into the limitations of your computer. You could of course buy bigger hard drives, more memory, and more powerful GPUs, but a better way to go about this is to use a dedicated compute cluster. This can either be a cluster from the Digital Research Alliance of Canada, your group's own server, or any of the commercial cloud offerings.\n",
    "\n",
    "While many of these offerings will come with JupyterHub interfaces of their own, it is not a very efficient way of doing things because Jupyter notebooks are not designed for efficiency. For example, they don't end when your code finishes running. For dedicated commercial clouds that means you will keep paying. For compute clusters, it will mean you keep resources tied up such that other users cannot use them.\n",
    "\n",
    "The solution to this is to use a standalone script that you start from the command-line. It will run from start to finish and save the results to disk. Afterwards, you will be able to load the saved model or results and analyze them on your own machine since those tasks would typically require much fewer resources than for training a model. This does mean you need to make a adjustment to your workflow. Before, you likely had a single notebook with both the training and the analysis in it which you will now have to split in two. There should be one notebook for training the model and writing the relevant files to disk and another notebook that loads those files and uses the trained model.\n",
    "\n",
    "There are a few ways of making a Jupyter notebook into a standalone entity. One of the easiest ways is to simply export from Jupyter by going to `File` → `Save and export notebook as...` → `Executable Script`.\n",
    "\n",
    "The resulting Python script is then something you can run from the command-line:\n",
    "```\n",
    "python nameofscript.py\n",
    "```\n",
    "\n",
    "This makes it suitable for running on remote servers. For example, for running on one of the Alliance clusters with a nice big GPU, you would create a submission script that look roughly like this:\n",
    "```\n",
    "#!/bin/bash\n",
    "#SBATCH --account=def-someuser    # Account name of your allocation. Usually def-[name of supervisor]\n",
    "#SBATCH --gpus-per-node=1         # Number of GPUs\n",
    "#SBATCH --mem=4000M               # Memory per node\n",
    "#SBATCH --time=0-03:00            # Maximum run time, in days - hours : minutes\n",
    "\n",
    "module load python                # Load the latest version of Python\n",
    "source ~/tf/bin/activate          # Load the Tensorflow or Torch environment\n",
    "python nameofscript.py            # Run your script\n",
    "```\n",
    "A good place to start to get familiar with HPC clusters is the <b> [HPC Intro](https://carpentries-incubator.github.io/hpc-intro/)</b> workshop. For submitting jobs on the Digital Research Alliance of Canada clusters, see <b>[GPU job submission](https://docs.alliancecan.ca/wiki/Using_GPUs_with_Slurm)</b> and <b>[Loading Tensorflow](https://docs.alliancecan.ca/wiki/TensorFlow)</b> or <b>[Loading PyTorch](https://docs.alliancecan.ca/wiki/PyTorch)</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b535827-f942-4305-aa82-b0298eb308c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb6d7aa-e550-4331-9313-c3a71d282025",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
